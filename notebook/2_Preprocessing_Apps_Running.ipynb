{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Preprocessing Data: Cleaning & Preparation üõ†Ô∏è  \n",
    "Setelah berhasil melakukan **scraping** data ulasan dari Google Play Store, langkah selanjutnya adalah **preprocessing**.  \n",
    "Tahap ini bertujuan untuk membersihkan dan menyiapkan data agar siap untuk analisis lebih lanjut.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **Libraries Used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from collections import Counter\n",
    "import langid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• **Import Data CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaa7089c-2edc-46de-84d1-ef6434a011f2</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>I really like it</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7.4</td>\n",
       "      <td>2025-03-12 09:42:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a3f09ed5-3e22-417a-9f95-f80c1fa752ea</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7.5</td>\n",
       "      <td>2025-03-12 07:20:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a02fb34e-14a1-4345-8b63-dc3b9ddd1f74</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7.4</td>\n",
       "      <td>2025-03-12 06:48:45</td>\n",
       "      <td>Your praise is the greatest encouragement to u...</td>\n",
       "      <td>2025-03-12 14:16:05</td>\n",
       "      <td>1.7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>958e76aa-ba71-4345-96c3-7578fa43126b</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>üëç</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7.4</td>\n",
       "      <td>2025-03-12 06:35:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0be62035-3d04-4d3d-b2a6-9efd5c48cbab</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>You can't swipe away notifications</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7.4</td>\n",
       "      <td>2025-03-12 06:17:30</td>\n",
       "      <td>Hi, thanks for your feedback. Could you send u...</td>\n",
       "      <td>2025-03-12 14:47:29</td>\n",
       "      <td>1.7.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId       userName  \\\n",
       "0  aaa7089c-2edc-46de-84d1-ef6434a011f2  A Google user   \n",
       "1  a3f09ed5-3e22-417a-9f95-f80c1fa752ea  A Google user   \n",
       "2  a02fb34e-14a1-4345-8b63-dc3b9ddd1f74  A Google user   \n",
       "3  958e76aa-ba71-4345-96c3-7578fa43126b  A Google user   \n",
       "4  0be62035-3d04-4d3d-b2a6-9efd5c48cbab  A Google user   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "1  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "2  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "3  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "4  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "\n",
       "                              content  score  thumbsUpCount  \\\n",
       "0                    I really like it      5              0   \n",
       "1                           Excellent      5              0   \n",
       "2                                Good      4              0   \n",
       "3                                   üëç      5              0   \n",
       "4  You can't swipe away notifications      1              0   \n",
       "\n",
       "  reviewCreatedVersion                   at  \\\n",
       "0                1.7.4  2025-03-12 09:42:30   \n",
       "1                1.7.5  2025-03-12 07:20:52   \n",
       "2                1.7.4  2025-03-12 06:48:45   \n",
       "3                1.7.4  2025-03-12 06:35:37   \n",
       "4                1.7.4  2025-03-12 06:17:30   \n",
       "\n",
       "                                        replyContent            repliedAt  \\\n",
       "0                                                NaN                  NaN   \n",
       "1                                                NaN                  NaN   \n",
       "2  Your praise is the greatest encouragement to u...  2025-03-12 14:16:05   \n",
       "3                                                NaN                  NaN   \n",
       "4  Hi, thanks for your feedback. Could you send u...  2025-03-12 14:47:29   \n",
       "\n",
       "  appVersion  \n",
       "0      1.7.4  \n",
       "1      1.7.5  \n",
       "2      1.7.4  \n",
       "3      1.7.4  \n",
       "4      1.7.4  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "file_path = \"../data/1_scrapping_running_tracker.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå **Tahapan Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **1. Handling Missing Values** ‚Üí Menghapus atau mengisi data yang kosong  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewId                    0\n",
      "userName                    0\n",
      "userImage                   0\n",
      "content                     1\n",
      "score                       0\n",
      "thumbsUpCount               0\n",
      "reviewCreatedVersion     2328\n",
      "at                          0\n",
      "replyContent            27722\n",
      "repliedAt               27722\n",
      "appVersion               2328\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah melakukan pengecekan missing values, ditemukan beberapa kolom dengan nilai kosong:  \n",
    "\n",
    "| **Kolom**               | **Jumlah Missing Values** | **Penanganan** |\n",
    "|-------------------------|-------------------------|---------------|\n",
    "| `content`              | 1                         | **Dihapus** (karena hanya 1 data) |\n",
    "| `reviewCreatedVersion` | 2,328                     | **Diisi** dengan `\"Unknown\"` atau modus |\n",
    "| `replyContent` & `repliedAt` | 27,722          | **Diisi** dengan `\"No Reply\"` |\n",
    "| `appVersion`           | 2,328                     | **Diisi** dengan modus atau `\"Unknown\"` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus baris dengan content kosong\n",
    "df = df.dropna(subset=[\"content\"])  \n",
    "\n",
    "# Menangani missing values dengan assignment langsung\n",
    "df[\"reviewCreatedVersion\"] = df[\"reviewCreatedVersion\"].fillna(\"Unknown\")\n",
    "df[\"replyContent\"] = df[\"replyContent\"].fillna(\"No Reply\")\n",
    "df[\"repliedAt\"] = df[\"repliedAt\"].fillna(\"No Reply\")\n",
    "df[\"appVersion\"] = df[\"appVersion\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewId                0\n",
      "userName                0\n",
      "userImage               0\n",
      "content                 0\n",
      "score                   0\n",
      "thumbsUpCount           0\n",
      "reviewCreatedVersion    0\n",
      "at                      0\n",
      "replyContent            0\n",
      "repliedAt               0\n",
      "appVersion              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **2. Expand Contractions** ‚Üí Mengubah singkatan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand contractions dilakukan sebelum tokenisasi karena banyak singkatan dalam bahasa Inggris yang mengandung apostrof (`'`), seperti `\"can't\"` menjadi `\"cannot\"` atau `\"you're\"` menjadi `\"you are\"`. Jika tokenisasi dilakukan terlebih dahulu, kata yang mengandung apostrof dapat terpisah menjadi token yang tidak bermakna, seperti `\"can't\"` menjadi `[\"ca\", \"n't\"]`. Dengan melakukan ekspansi terlebih dahulu, kita memastikan bahwa setiap kata tetap utuh sebelum diproses lebih lanjut dalam analisis teks, sehingga hasil tokenisasi menjadi lebih akurat dan sesuai dengan makna sebenarnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_expanded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really like it</td>\n",
       "      <td>I really like it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üëç</td>\n",
       "      <td>üëç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You can't swipe away notifications</td>\n",
       "      <td>You cannot swipe away notifications</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              content                     content_expanded\n",
       "0                    I really like it                     I really like it\n",
       "1                           Excellent                            Excellent\n",
       "2                                Good                                 Good\n",
       "3                                   üëç                                    üëç\n",
       "4  You can't swipe away notifications  You cannot swipe away notifications"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"Mengubah singkatan dalam teks menjadi bentuk lengkapnya.\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "df[\"content_expanded\"] = df[\"content\"].apply(expand_contractions)\n",
    "df[[\"content\", \"content_expanded\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **3. Tokenization** ‚Üí Memisahkan teks menjadi kata-kata atau unit kecil lainnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam proyek ini, tokenisasi dilakukan **menggunakan BERT tokenizer** untuk memproses teks ulasan aplikasi sebelum analisis sentimen. BERT (Bidirectional Encoder Representations from Transformers) dipilih karena memiliki **bidirectional context** , yang memungkinkan model memahami makna kata berdasarkan keseluruhan kalimat, serta **pre-trained model**, yang telah dilatih dengan miliaran kata sehingga meningkatkan akurasi pemrosesan teks. Selain itu, BERT sangat **cocok untuk analisis sentimen**, karena mampu menangkap nuansa emosi dalam teks lebih baik dibandingkan metode tokenisasi tradisional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi menggunakan BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "df[\"tokens\"] = df[\"content_expanded\"].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_expanded</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really like it</td>\n",
       "      <td>[i, really, like, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>[excellent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üëç</td>\n",
       "      <td>[[UNK]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You cannot swipe away notifications</td>\n",
       "      <td>[you, cannot, sw, ##ipe, away, notification, ##s]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      content_expanded  \\\n",
       "0                     I really like it   \n",
       "1                            Excellent   \n",
       "2                                 Good   \n",
       "3                                    üëç   \n",
       "4  You cannot swipe away notifications   \n",
       "\n",
       "                                              tokens  \n",
       "0                              [i, really, like, it]  \n",
       "1                                        [excellent]  \n",
       "2                                             [good]  \n",
       "3                                            [[UNK]]  \n",
       "4  [you, cannot, sw, ##ipe, away, notification, ##s]  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek hasil tokenisasi\n",
    "df[[\"content_expanded\", \"tokens\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, really, like, it]</td>\n",
       "      <td>[i, really, like, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[excellent]</td>\n",
       "      <td>[excellent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[good]</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[UNK]]</td>\n",
       "      <td>[[UNK]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, cannot, sw, ##ipe, away, notification, ##s]</td>\n",
       "      <td>[you, cannot, swipe, away, notifications]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0                              [i, really, like, it]   \n",
       "1                                        [excellent]   \n",
       "2                                             [good]   \n",
       "3                                            [[UNK]]   \n",
       "4  [you, cannot, sw, ##ipe, away, notification, ##s]   \n",
       "\n",
       "                               tokens_merged  \n",
       "0                      [i, really, like, it]  \n",
       "1                                [excellent]  \n",
       "2                                     [good]  \n",
       "3                                    [[UNK]]  \n",
       "4  [you, cannot, swipe, away, notifications]  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menggabungkan kata yang terpisah oleh ##\n",
    "\n",
    "def merge_subwords(tokens):\n",
    "    merged_tokens = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += token[2:]  # Gabungkan tanpa '##'\n",
    "        else:\n",
    "            if current_word:\n",
    "                merged_tokens.append(current_word)  # Simpan kata sebelumnya\n",
    "            current_word = token  # Mulai kata baru\n",
    "\n",
    "    if current_word:  # Pastikan kata terakhir masuk\n",
    "        merged_tokens.append(current_word)\n",
    "\n",
    "    return merged_tokens\n",
    "\n",
    "df[\"tokens_merged\"] = df[\"tokens\"].apply(merge_subwords)\n",
    "df[[\"tokens\", \"tokens_merged\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **4. Lowercasing** ‚Üí Mengubah teks menjadi huruf kecil untuk konsistensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_merged</th>\n",
       "      <th>tokens_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, really, like, it]</td>\n",
       "      <td>[i, really, like, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[excellent]</td>\n",
       "      <td>[excellent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[good]</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[unk]]</td>\n",
       "      <td>[[unk]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, cannot, swipe, away, notifications]</td>\n",
       "      <td>[you, cannot, swipe, away, notifications]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               tokens_merged  \\\n",
       "0                      [i, really, like, it]   \n",
       "1                                [excellent]   \n",
       "2                                     [good]   \n",
       "3                                    [[unk]]   \n",
       "4  [you, cannot, swipe, away, notifications]   \n",
       "\n",
       "                               tokens_merged  \n",
       "0                      [i, really, like, it]  \n",
       "1                                [excellent]  \n",
       "2                                     [good]  \n",
       "3                                    [[unk]]  \n",
       "4  [you, cannot, swipe, away, notifications]  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokens_merged\"] = df[\"tokens_merged\"].apply(lambda x: [token.lower() for token in x])\n",
    "\n",
    "# Cek hasil lowercasing\n",
    "df[[\"tokens_merged\", \"tokens_merged\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **5. Stopword Removal** ‚Üí Menghapus kata-kata umum yang tidak bermakna dalam analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_merged</th>\n",
       "      <th>tokens_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, really, like, it]</td>\n",
       "      <td>[really, like]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[excellent]</td>\n",
       "      <td>[excellent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[good]</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[unk]]</td>\n",
       "      <td>[[unk]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, cannot, swipe, away, notifications]</td>\n",
       "      <td>[cannot, swipe, away, notifications]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               tokens_merged  \\\n",
       "0                      [i, really, like, it]   \n",
       "1                                [excellent]   \n",
       "2                                     [good]   \n",
       "3                                    [[unk]]   \n",
       "4  [you, cannot, swipe, away, notifications]   \n",
       "\n",
       "                         tokens_cleaned  \n",
       "0                        [really, like]  \n",
       "1                           [excellent]  \n",
       "2                                [good]  \n",
       "3                               [[unk]]  \n",
       "4  [cannot, swipe, away, notifications]  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "important_words = {\"not\", \"no\", \"never\", \"nor\", \"n't\"}  \n",
    "filtered_stop_words = stop_words - important_words  \n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    cleaned_tokens = [\n",
    "        token.strip() for token in tokens  # Hapus spasi/karakter aneh\n",
    "        if token.lower() not in filtered_stop_words and len(token) > 2  # Hapus hanya stop words yang tidak penting\n",
    "    ]\n",
    "    return cleaned_tokens\n",
    "\n",
    "df[\"tokens_cleaned\"] = df[\"tokens_merged\"].apply(remove_stopwords)\n",
    "df[[\"tokens_merged\", \"tokens_cleaned\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **6.  Lemmatization** ‚Üí Mengubah kata menjadi bentuk dasarnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam proses ini, kita menggunakan **Lemmatization** untuk mengubah kata ke bentuk dasarnya berdasarkan konteks linguistik. Berbeda dengan **Stemming** yang hanya memotong kata tanpa memperhatikan makna, **Lemmatization mempertahankan makna asli kata**. **BERT Tokenizer sudah memahami konteks kata**, sehingga proses Stemming justru bisa menghilangkan informasi penting yang diperlukan dalam analisis sentimen.  Jadi, proses stemming dilewati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <th>tokens_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[really, like]</td>\n",
       "      <td>[really, like]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[excellent]</td>\n",
       "      <td>[excellent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[good]</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[unk]]</td>\n",
       "      <td>[[unk]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[cannot, swipe, away, notifications]</td>\n",
       "      <td>[cannot, swipe, away, notifications]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         tokens_cleaned                     tokens_lemmatized\n",
       "0                        [really, like]                        [really, like]\n",
       "1                           [excellent]                           [excellent]\n",
       "2                                [good]                                [good]\n",
       "3                               [[unk]]                               [[unk]]\n",
       "4  [cannot, swipe, away, notifications]  [cannot, swipe, away, notifications]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token, pos=\"v\") for token in tokens]  # 'v' untuk verb\n",
    "\n",
    "df[\"tokens_lemmatized\"] = df[\"tokens_cleaned\"].apply(lemmatize_tokens)\n",
    "df[[\"tokens_cleaned\", \"tokens_lemmatized\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **7. Punctuation & Special Characters Removal** ‚Üí Menghapus tanda baca dan simbol yang tidak relevan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_lemmatized</th>\n",
       "      <th>tokens_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[really, like]</td>\n",
       "      <td>[really, like]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[excellent]</td>\n",
       "      <td>[excellent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[good]</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[unk]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[cannot, swipe, away, notifications]</td>\n",
       "      <td>[cannot, swipe, away, notifications]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      tokens_lemmatized                        tokens_cleaned\n",
       "0                        [really, like]                        [really, like]\n",
       "1                           [excellent]                           [excellent]\n",
       "2                                [good]                                [good]\n",
       "3                               [[unk]]                                    []\n",
       "4  [cannot, swipe, away, notifications]  [cannot, swipe, away, notifications]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_tokens(tokens):\n",
    "    \"\"\"Menghapus tanda baca dan token [UNK]\"\"\"\n",
    "    return [token for token in tokens if token not in string.punctuation and token.lower() != \"[unk]\"]\n",
    "\n",
    "df[\"tokens_cleaned\"] = df[\"tokens_lemmatized\"].apply(clean_tokens)\n",
    "df[[\"tokens_lemmatized\", \"tokens_cleaned\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **8. Spelling Correction** ‚Üí Memperbaiki kesalahan ejaan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk meningkatkan akurasi analisis teks, saya menggunakan **SymSpell** sebagai metode *spelling correction*. SymSpell adalah algoritma koreksi ejaan yang sangat cepat karena menggunakan teknik pencocokan berbasis hash dan precomputed edit distance.  \n",
    "\n",
    "**kamus frekuensi kata berbahasa Inggris** dari repositori SymSpell:  \n",
    "[SymSpell Frequency Dictionary](https://github.com/wolfgarbe/SymSpell/blob/master/SymSpell/frequency_dictionary_en_82_765.txt)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <th>tokens_spelling_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[veryyy, goood]</td>\n",
       "      <td>[very, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[nice, feature, simple, use, dependable, gps]</td>\n",
       "      <td>[nice, feature, simple, use, dependable, gas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[nice, aap]</td>\n",
       "      <td>[nice, map]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[nice, best, app, run, ads, funcions, overall,...</td>\n",
       "      <td>[nice, best, app, run, ads, functions, overall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[good, app, keep, calculate, wrond, distance, ...</td>\n",
       "      <td>[good, app, keep, calculate, wrong, distance, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tokens_cleaned  \\\n",
       "9                                     [veryyy, goood]   \n",
       "10      [nice, feature, simple, use, dependable, gps]   \n",
       "12                                        [nice, aap]   \n",
       "37  [nice, best, app, run, ads, funcions, overall,...   \n",
       "48  [good, app, keep, calculate, wrond, distance, ...   \n",
       "\n",
       "                            tokens_spelling_corrected  \n",
       "9                                        [very, good]  \n",
       "10      [nice, feature, simple, use, dependable, gas]  \n",
       "12                                        [nice, map]  \n",
       "37  [nice, best, app, run, ads, functions, overall...  \n",
       "48  [good, app, keep, calculate, wrong, distance, ...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# Load dictionary\n",
    "dictionary_path = \"frequency_dictionary_en_82_765.txt\"  \n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "    print(\"Gagal memuat dictionary. Pastikan file dictionary tersedia di path yang benar.\")\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    corrected_tokens = []\n",
    "    for token in tokens:\n",
    "        suggestion = sym_spell.lookup(token, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestion:\n",
    "            corrected_tokens.append(suggestion[0].term)\n",
    "        else:\n",
    "            corrected_tokens.append(token)  # Jika tidak ada koreksi, gunakan kata asli\n",
    "    return corrected_tokens\n",
    "\n",
    "# Terapkan koreksi ejaan pada token yang sudah dibersihkan\n",
    "df[\"tokens_spelling_corrected\"] = df[\"tokens_cleaned\"].apply(correct_spelling)\n",
    "\n",
    "# Tampilkan hanya baris yang mengalami perubahan\n",
    "df_changes = df[df[\"tokens_cleaned\"].astype(str) != df[\"tokens_spelling_corrected\"].astype(str)]\n",
    "df_changes[[\"tokens_cleaned\", \"tokens_spelling_corrected\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **9. Rare Words Removal** ‚Üí Menghapus kata yang jarang muncul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahap ini, kata-kata yang jarang muncul dalam dataset akan dihapus. Rare words sering kali tidak memberikan kontribusi signifikan dalam analisis sentimen karena frekuensinya yang sangat rendah dan cenderung menjadi noise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_spelling_corrected</th>\n",
       "      <th>tokens_rare_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[cannot, swipe, away, notifications]</td>\n",
       "      <td>[cannot, away, notifications]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[nice, feature, simple, use, dependable, gas]</td>\n",
       "      <td>[nice, feature, simple, use, gas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[weight, loss, bat, application]</td>\n",
       "      <td>[weight, loss, application]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[nice, best, app, run, ads, functions, overall...</td>\n",
       "      <td>[nice, best, app, run, ads, overall, free]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>[exemplary, good]</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            tokens_spelling_corrected  \\\n",
       "4                [cannot, swipe, away, notifications]   \n",
       "10      [nice, feature, simple, use, dependable, gas]   \n",
       "17                   [weight, loss, bat, application]   \n",
       "37  [nice, best, app, run, ads, functions, overall...   \n",
       "55                                  [exemplary, good]   \n",
       "\n",
       "                           tokens_rare_removed  \n",
       "4                [cannot, away, notifications]  \n",
       "10           [nice, feature, simple, use, gas]  \n",
       "17                 [weight, loss, application]  \n",
       "37  [nice, best, app, run, ads, overall, free]  \n",
       "55                                      [good]  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter(word for tokens in df[\"tokens_spelling_corrected\"] for word in tokens)\n",
    "\n",
    "# Tentukan threshold (kata muncul kurang dari 5 kali akan dihapus)\n",
    "rare_threshold = 5  \n",
    "rare_words = {word for word, count in word_counts.items() if count < rare_threshold}\n",
    "\n",
    "# Fungsi untuk menghapus rare words dari token\n",
    "def remove_rare_words(tokens):\n",
    "    return [word for word in tokens if word not in rare_words]\n",
    "\n",
    "df[\"tokens_rare_removed\"] = df[\"tokens_spelling_corrected\"].apply(remove_rare_words)\n",
    "\n",
    "df_changes = df[df[\"tokens_spelling_corrected\"].astype(str) != df[\"tokens_rare_removed\"].astype(str)]\n",
    "df_changes[[\"tokens_spelling_corrected\", \"tokens_rare_removed\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam analisis sentimen ulasan aplikasi, penghapusan kata-kata umum (Common Words Removal) **tidak dilakukan** karena:  \n",
    "\n",
    "1. **Stopwords sudah dihapus lebih awal** ‚Üí Kata-kata umum yang tidak bermakna sudah dihilangkan melalui proses stopwords removal.  \n",
    "2. **Kata-kata umum dapat memiliki makna dalam sentimen** ‚Üí Kata seperti *\"good\"*, *\"bad\"*, atau *\"great\"* penting untuk menentukan polaritas sentimen.  \n",
    "3. **Menghindari kehilangan konteks** ‚Üí Jika terlalu banyak kata dihapus, makna asli dari ulasan bisa berubah atau berkurang.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **10. Language Detection & Filtering** ‚Üí Mendeteksi bahasa dan menghapus selain bahasa Inggris.\n",
    "\n",
    "Langkah selanjutnya adalah **mendeteksi bahasa** pada setiap ulasan.  \n",
    "Karena analisis ini difokuskan pada ulasan berbahasa Inggris, maka ulasan yang terdeteksi menggunakan bahasa lain akan **dihapus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "en    24030\n",
      "de     1218\n",
      "sv     1041\n",
      "nl      440\n",
      "fr      425\n",
      "es      288\n",
      "no      214\n",
      "it      183\n",
      "nb      180\n",
      "da      167\n",
      "mt       94\n",
      "ms       64\n",
      "pl       61\n",
      "fi       54\n",
      "eu       50\n",
      "lv       50\n",
      "lb       49\n",
      "id       42\n",
      "fa       37\n",
      "af       35\n",
      "et       27\n",
      "ca       23\n",
      "ar       21\n",
      "pt       21\n",
      "ro       17\n",
      "sl       13\n",
      "nn       11\n",
      "br        9\n",
      "oc        9\n",
      "tl        8\n",
      "bg        7\n",
      "ru        7\n",
      "wa        5\n",
      "hu        5\n",
      "hi        5\n",
      "sw        5\n",
      "vi        4\n",
      "eo        3\n",
      "la        2\n",
      "cs        2\n",
      "is        2\n",
      "ht        1\n",
      "sk        1\n",
      "cy        1\n",
      "tr        1\n",
      "ga        1\n",
      "gl        1\n",
      "lt        1\n",
      "Name: count, dtype: int64\n",
      "                                             content language\n",
      "1                                          Excellent       de\n",
      "8                                               ÿÆŸàÿ®Ÿá       fa\n",
      "17                     Weight loss bat application üòö       es\n",
      "25  One of the best app, simple and straight forward       sv\n",
      "26     very good app motivate everyday for running üòÄ       no\n"
     ]
    }
   ],
   "source": [
    "def detect_language_langid(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    return langid.classify(text)[0]  # Ambil kode bahasa \n",
    "\n",
    "df = df.copy()\n",
    "df.loc[:, \"language\"] = df[\"tokens_rare_removed\"].apply(detect_language_langid)\n",
    "\n",
    "print(df[\"language\"].value_counts())\n",
    "\n",
    "df_non_english = df[df[\"language\"] != \"en\"]\n",
    "print(df_non_english[[\"content\", \"language\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah ulasan setelah filter hanya bahasa Inggris: 24030\n"
     ]
    }
   ],
   "source": [
    "# Hapus baris yang bukan bahasa Inggris\n",
    "df = df[df[\"language\"] == \"en\"].copy()\n",
    "\n",
    "# Tampilkan jumlah ulasan setelah penghapusan\n",
    "print(f\"Jumlah ulasan setelah filter hanya bahasa Inggris: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **11 .Delete Empty Rows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika sebuah review kehilangan katanya setelah proses preprocessing ini, maka baris tersebut tidak lagi memiliki informasi yang berguna. Oleh karena itu, baris-baris kosong akan dihapus agar dataset tetap bersih dan bermakna untuk analisis sentimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris kosong: 1515\n"
     ]
    }
   ],
   "source": [
    "# Cek jumlah baris yang memiliki token kosong setelah preprocessing\n",
    "df_empty = df[df[\"tokens_rare_removed\"].apply(lambda x: len(x) == 0)]\n",
    "print(f\"Jumlah baris kosong: {len(df_empty)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah ulasan setelah menghapus baris kosong: 22515\n"
     ]
    }
   ],
   "source": [
    "# Hapus baris yang kosong\n",
    "df = df[df[\"tokens_rare_removed\"].apply(lambda x: len(x) > 0)].copy()\n",
    "\n",
    "# Tampilkan jumlah ulasan setelah penghapusan\n",
    "print(f\"Jumlah ulasan setelah menghapus baris kosong: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå **Save Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data berhasil disimpan ke ../data/2_preprocess_running_tracker.csv\n"
     ]
    }
   ],
   "source": [
    "# Simpan hanya konten yang telah dibersihkan\n",
    "df[\"content_cleaned\"] = df[\"tokens_rare_removed\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Simpan DataFrame ke dalam folder data dengan path relatif \n",
    "output_path = \"../data/2_preprocess_running_tracker.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Data berhasil disimpan ke {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data berhasil disimpan ke ../data/2_cleandata_running_tracker.csv\n"
     ]
    }
   ],
   "source": [
    "columns_to_keep = [\"reviewId\", \"userName\", \"userImage\", \"score\", \"thumbsUpCount\", \n",
    "                   \"reviewCreatedVersion\", \"at\", \"replyContent\", \"repliedAt\", \n",
    "                   \"appVersion\", \"content_cleaned\"]  \n",
    "\n",
    "# Buat DataFrame baru dengan hanya kolom yang diperlukan\n",
    "df_final = df[columns_to_keep].copy()\n",
    "\n",
    "# Simpan DataFrame ke dalam folder data dengan path relatif\n",
    "output_path = \"../data/2_cleandata_running_tracker.csv\"\n",
    "df_final.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Data berhasil disimpan ke {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310",
   "language": "python",
   "name": "env310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
